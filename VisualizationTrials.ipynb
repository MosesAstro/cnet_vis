{
 "metadata": {
  "name": "",
  "signature": "sha256:fa82d8a215f09fab23817d2e532b19b5f4b85f9c89be04296a3b11f2d5178848"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab osx\n",
      "try:\n",
      "    reload(pvlparser)\n",
      "except:\n",
      "    \n",
      "    import pvlparser\n",
      "from ast import literal_eval\n",
      "from collections import OrderedDict, defaultdict\n",
      "from itertools import islice\n",
      "import json\n",
      "import re\n",
      "\n",
      "import numpy as np\n",
      "import tables as tb\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "import ControlNetFileV0002_pb2 as spec\n",
      "\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inpvl = ''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##PVL Mapper\n",
      "\n",
      "Convert the next cell back to code to read a PVL file into a JSON representation."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "fileend = re.compile(r\"\\bEnd\\b\", re.IGNORECASE)\n",
      "objstart = re.compile(r\"\\s*\\bObject\\b\")\n",
      "objend = re.compile(r\"\\s*\\bEnd_Object\\b\")\n",
      "groupstart = re.compile(r\"\\s*Group\\b\")\n",
      "groupend = re.compile(r\"\\s*End_Group\\b\")\n",
      "\n",
      "comment = re.compile(r\"\\s*#\")\n",
      "altcomment = re.compile(r\"\\s*/\\*\")\n",
      "whitespace = re.compile(r\"^\\s*$\")\n",
      "\n",
      "cnet = OrderedDict()\n",
      "cnet['ControlPoints'] = []\n",
      "with open(inpvl, 'r') as f:\n",
      "    for i, line in enumerate(f):\n",
      "        if whitespace.match(line):\n",
      "            continue\n",
      "        elif fileend.match(line):\n",
      "            break\n",
      "        elif groupend.match(line):\n",
      "            current_load_point = previous_load_point\n",
      "        elif objend.match(line):\n",
      "            continue\n",
      "        elif objstart.match(line):\n",
      "            objkey = re.split('=', line.rstrip())[-1].strip()\n",
      "            if objkey == 'ControlNetwork':\n",
      "                cnet['ControlNetwork'] = {}\n",
      "                current_load_point = cnet['ControlNetwork']\n",
      "            elif objkey == 'ControlPoint':\n",
      "                cnet['ControlPoints'].append({})\n",
      "                current_load_point = cnet['ControlPoints'][-1]\n",
      "        elif groupstart.match(line):\n",
      "            previous_load_point = current_load_point\n",
      "            grpkey = re.split('=', line.rstrip())[-1].strip()            \n",
      "            if grpkey not in current_load_point:\n",
      "                current_load_point[grpkey] = [{}]\n",
      "            else:\n",
      "                current_load_point[grpkey].append({})\n",
      "            \n",
      "            current_load_point = current_load_point[grpkey][-1]\n",
      "        else:\n",
      "            data = re.split('=', line.rstrip())\n",
      "            base = data[1].strip()\n",
      "            if data[1].endswith('-'):\n",
      "                continuation = True\n",
      "                while continuation:\n",
      "                    for l in islice(f,1):\n",
      "                        base = base[:-1] + l.rstrip().strip()\n",
      "                        if not l.endswith('-'):\n",
      "                            continuation = False\n",
      "            elif data[1].endswith(','):\n",
      "                continuation = True\n",
      "                while continuation:\n",
      "                    for l in islice(f, 1):\n",
      "                        base = base + l.rstrip().strip()\n",
      "                        if l.rstrip().endswith(')'):\n",
      "                            continuation = False\n",
      "            try:\n",
      "                base = literal_eval(base.split('<')[0].strip())\n",
      "            except:\n",
      "                pass\n",
      "            if isinstance(current_load_point, dict):\n",
      "                current_load_point[data[0].strip()] = base\n",
      "            elif isinstance(current_load_point, list):\n",
      "                current_load_point.append({data[0].strip():base})        "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "#PVL PARSER\n",
      "t1 = time.time()\n",
      "fp = freetable.row\n",
      "cp = constrainedtable.row\n",
      "for c in cnet['ControlPoints']:\n",
      "    for j in c['ControlMeasure']:\n",
      "        #if c['PointType'] == 'Constrained':\n",
      "            #crow = cp\n",
      "        #elif c['PointType'] == 'Free':\n",
      "        crow = fp\n",
      "\n",
      "        #Per Control Point\n",
      "        crow['pointtype'] = c['PointType']\n",
      "        crow['pointid'] = c['PointId']\n",
      "        crow['apriorilatitude'] = c['# AprioriLatitude']\n",
      "        crow['apriorix'] = c['AprioriX']\n",
      "        crow['apriorilongitude'] = c['# AprioriLongitude']\n",
      "        crow['aprioriy'] = c['AprioriY']\n",
      "        crow['aprioriradius'] =  c['# AprioriRadius']\n",
      "        crow['aprioriz'] = c['AprioriZ']\n",
      "\n",
      "        crow['adjustedlatitude'] = c['# AdjustedLatitude']\n",
      "        crow['adjustedlongitude'] = c['# AdjustedLongitude']\n",
      "        crow['adjustedradius'] = c['# AdjustedRadius']\n",
      "        crow['adjustedx'] = c['AdjustedX']\n",
      "        crow['adjustedy'] = c['AdjustedY']\n",
      "        crow['adjustedz'] = c['AdjustedZ']\n",
      "        \n",
      "        #Per Measure\n",
      "        crow['serialnumber'] = j['SerialNumber']\n",
      "        crow['measuretype']= j['MeasureType']\n",
      "        crow['choosername'] = j['ChooserName']\n",
      "        crow['sample'] = j['Sample']\n",
      "        crow['line'] = j['Line']\n",
      "        try:\n",
      "            crow['sampleresidual'] = j['SampleResidual']\n",
      "        except:\n",
      "            crow['sampleresidual'] = np.NaN\n",
      "        try:\n",
      "            crow['lineresidual'] = j['LineResidual']\n",
      "        except:\n",
      "            crow['lineresidual'] = np.NaN\n",
      "        try:\n",
      "            crow['goodnessoffit'] = j['GoodnessOfFit']\n",
      "        except:\n",
      "            crow['goodnessoffit'] = np.NaN\n",
      "        crow.append()\n",
      "\n",
      "    freetable.flush()\n",
      "constrainedtable.flush()\n",
      "h5file.close()\n",
      "t2 = time.time()\n",
      "print t2 - t1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Sanity Check\n",
      "ncontrolpoints = len(cnet['ControlPoints'])\n",
      "print ncontrolpoints"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "256688\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Mapping of the protobuf control network into an HDF5 table."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class ControlPoint(tb.IsDescription):\n",
      "    pointid = tb.StringCol(64)  #Why string?  #Int or float?  In the latter case, \n",
      "\n",
      "    pointtype = tb.StringCol(12)\n",
      "    choosername = tb.StringCol(36)\n",
      "    datetime = tb.StringCol(36)  #Should be datetime\n",
      "    referenceindex = tb.Int32Col()\n",
      "    apriorisurface_pointsource = tb.StringCol(32)\n",
      "    apriorisurface_pointfile = tb.StringCol(128)\n",
      "    aprioriradius_pointsource = tb.StringCol(32)\n",
      "    aprioriradius_pointfile = tb.StringCol(128)\n",
      "    \n",
      "    latitudeconstrained = tb.BoolCol()\n",
      "    longitudeconstrained = tb.BoolCol()\n",
      "    radiusconstrained = tb.BoolCol()\n",
      "    \n",
      "    #apriorilatitude = tb.FloatCol()\n",
      "    apriorix = tb.FloatCol()\n",
      "    #apriorilongitude = tb.FloatCol()\n",
      "    aprioriy = tb.FloatCol()\n",
      "    #aprioriradius = tb.FloatCol()\n",
      "    aprioriz = tb.FloatCol()\n",
      "    \n",
      "    #adjustedlatitude = tb.FloatCol()\n",
      "    adjustedx = tb.FloatCol()\n",
      "    #adjustedlongitude = tb.FloatCol()\n",
      "    adjustedy = tb.FloatCol()\n",
      "    #adjustedradius = tb.FloatCol()\n",
      "    adjustedz = tb.FloatCol()\n",
      "\n",
      "    #aprioricovar = tb.Float32Col(shape=6)\n",
      "    #adjustedcovar = tb.Float32Col(shape=6)\n",
      "\n",
      "    #Measure\n",
      "    serialnumber = tb.StringCol(256)\n",
      "    measuretype = tb.StringCol(32)\n",
      "    choosername = tb.StringCol(32)\n",
      "    datetime = tb.StringCol(64)  #should be datetime\n",
      "    editlock = tb.BoolCol()\n",
      "    ignore = tb.BoolCol()\n",
      "    jigsawrejected = tb.BoolCol()\n",
      "    diameter = tb.FloatCol()\n",
      "    sample = tb.FloatCol()\n",
      "    line = tb.FloatCol()\n",
      "    \n",
      "    samplesigma = tb.FloatCol()\n",
      "    linesigma = tb.FloatCol()\n",
      "    \n",
      "    apriorisample = tb.FloatCol()\n",
      "    aprioriline = tb.FloatCol()\n",
      "    sampleresidual = tb.FloatCol()\n",
      "    lineresidual = tb.FloatCol()\n",
      "    #goodnessoffit = tb.FloatCol()\n",
      "    #reference = tb.BoolCol()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 124
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Define HDF5 structure - Do not run....legacy from parsing PVL...\n",
      "\n",
      "Convert the following cell back to code in order to delete an existing hdf5file and generate an empty h5 container."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "try:\n",
      "    h5file.close()\n",
      "except:\n",
      "    pass\n",
      "outfile = 'largehdf5.h5'\n",
      "h5file = tb.open_file(outfile, mode='w', title='cnet')\n",
      "image_group = h5file.create_group('/', 'Images', 'Lookup table of images to image serials')\n",
      "\n",
      "#Maybe kick down a level\n",
      "free_group = h5file.create_group('/', 'Free', 'Points of type free')\n",
      "fixed_group = h5file.create_group('/', 'Fixed', 'Points of type fixed')\n",
      "constrained_group = h5file.create_group('/', 'Constrained', 'Points of type constrained')\n",
      "\n",
      "#Table within groups\n",
      "freetable = h5file.create_table(free_group, 'controlmeasure', ControlPoint, 'Free Control Points')\n",
      "constrainedtable = h5file.create_table(constrained_group, 'controlmeasure', ControlPoint, 'Constrainted Control Points')\n",
      "fixedtable = h5file.create_table(fixed_group, 'controlmeasure', ControlPoint, 'Fixed Control Points')"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Write the Protocol Buffer representation into the hdf5 container\n",
      "\n",
      "I think that this can be significantly faster using the Pandas storer class."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t1 = time.time()\n",
      "\n",
      "try:\n",
      "    h5file.close()\n",
      "except:\n",
      "    pass\n",
      "outfile = 'elysium.h5'\n",
      "h5file = tb.open_file(outfile, mode='w', title='cnet')\n",
      "image_group = h5file.create_group('/', 'Images', 'Lookup table of images to image serials')\n",
      "\n",
      "#Maybe kick down a level\n",
      "free_group = h5file.create_group('/', 'Free', 'Points of type free')\n",
      "#fixed_group = h5file.create_group('/', 'Fixed', 'Points of type fixed')\n",
      "#constrained_group = h5file.create_group('/', 'Constrained', 'Points of type constrained')\n",
      "\n",
      "#Table within groups\n",
      "freetable = h5file.create_table(free_group, 'controlmeasure', ControlPoint, 'Free Control Points')\n",
      "#constrainedtable = h5file.create_table(constrained_group, 'controlmeasure', ControlPoint, 'Constrainted Control Points')\n",
      "#fixedtable = h5file.create_table(fixed_group, 'controlmeasure', ControlPoint, 'Fixed Control Points')\n",
      "\n",
      "#For now - pack all points into a single free table.\n",
      "\n",
      "#Create a row object in the free table.\n",
      "freerow = freetable.row\n",
      "\n",
      "#Open the input dataset\n",
      "with open('Elysium_DayIR_Final.net', 'r') as stream:\n",
      "    header = spec.ControlNetFileHeaderV0002()\n",
      "    stream.seek(65536)  #Why are we doing more custom stuff?\n",
      "    headerbinary = stream.read(513513)\n",
      "    header.ParseFromString(headerbinary)\n",
      "    binarysizes = header.pointMessageSizes\n",
      "    for i, chunk in enumerate(binarysizes):\n",
      "        #Read the message\n",
      "        binary = stream.read(chunk)\n",
      "        cp = spec.ControlPointFileEntryV0002()\n",
      "        cp.ParseFromString(binary)\n",
      "        \n",
      "        #Loop over the points\n",
      "        for j, m in enumerate(cp.measures):\n",
      "            \n",
      "            freerow['pointid'] = cp.id\n",
      "            freerow['pointtype'] = cp.type\n",
      "            freerow['choosername'] = cp.chooserName\n",
      "            #datetime = tb.StringCol(36)  #Should be datetime\n",
      "            freerow['referenceindex'] = cp.referenceIndex\n",
      "            freerow['apriorisurface_pointsource'] = cp.aprioriSurfPointSource\n",
      "            freerow['apriorisurface_pointfile'] = cp.aprioriSurfPointSourceFile\n",
      "            freerow['aprioriradius_pointsource'] = cp.aprioriRadiusSource\n",
      "            freerow['aprioriradius_pointfile'] = cp.aprioriRadiusSourceFile\n",
      "\n",
      "            freerow['latitudeconstrained'] = cp.latitudeConstrained\n",
      "            freerow['longitudeconstrained'] = cp.longitudeConstrained\n",
      "            freerow['radiusconstrained'] = cp.radiusConstrained\n",
      "\n",
      "            #apriorilatitude = cp.\n",
      "            freerow['apriorix'] = cp.aprioriX\n",
      "            #apriorilongitude = tb.FloatCol()\n",
      "            freerow['aprioriy'] = cp.aprioriY\n",
      "            #aprioriradius = tb.FloatCol()\n",
      "            freerow['aprioriz'] = cp.aprioriZ\n",
      "\n",
      "            #adjustedlatitude = tb.FloatCol()\n",
      "            freerow['adjustedx'] = cp.adjustedX\n",
      "            #adjustedlongitude = tb.FloatCol()\n",
      "            freerow['adjustedy'] = cp.adjustedY\n",
      "            #adjustedradius = tb.FloatCol()\n",
      "            freerow['adjustedz'] = cp.adjustedZ\n",
      "            \n",
      "            '''\n",
      "            arr = cp.aprioriCovar\n",
      "            if len(arr) != 6:\n",
      "                freerow['aprioricovar'] = np.zeros(6)\n",
      "            else:\n",
      "                freerow['aprioricovar'] = arr\n",
      "            print freerow['aprioricovar']\n",
      "            \n",
      "            arr = cp.adjustedCovar\n",
      "            if len(arr) != 6:\n",
      "                freerow['adjustedcovar'] = np.zeros(6)\n",
      "            else:\n",
      "                freerow['adjustedcovar'] = arr\n",
      "\n",
      "            print freerow['adjustedcovar']\n",
      "            '''\n",
      "            \n",
      "            freerow['serialnumber'] = m.serialnumber\n",
      "            freerow['measuretype'] = m.type\n",
      "            freerow['choosername'] = m.choosername\n",
      "            #datetime = tb.StringCol(64)  #should be datetime\n",
      "            freerow['editlock'] = m.editLock\n",
      "            freerow['ignore'] = m.ignore\n",
      "            freerow['jigsawrejected'] = m.jigsawRejected\n",
      "            freerow['diameter'] = m.diameter\n",
      "            freerow['sample'] = m.sample\n",
      "            freerow['line'] = m.line\n",
      "\n",
      "            freerow['samplesigma'] = m.samplesigma\n",
      "            freerow['linesigma'] = m.linesigma\n",
      "            \n",
      "            freerow['apriorisample'] = m.apriorisample\n",
      "            freerow['aprioriline'] = m.aprioriline\n",
      "            freerow['sampleresidual'] = m.sampleResidual\n",
      "            freerow['lineresidual'] = m.lineResidual\n",
      "\n",
      "            #Append the row to the table.  This is a bulk insert style\n",
      "            freerow.append()\n",
      "        #Flush the table to explicitly write to disk\n",
      "        freetable.flush()\n",
      "t2 = time.time()\n",
      "print 'Protobuf to hdf5 took {}'.format(t2-t1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Protobuf to hdf5 took 171.725105047\n"
       ]
      }
     ],
     "prompt_number": 125
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Load h5 file into Pandas"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print outfile\n",
      "t1 = time.time()\n",
      "hdf = pd.read_hdf(outfile, '/Free/controlmeasure')\n",
      "t2 = time.time()\n",
      "print t2 - t1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "elysium.h5\n",
        "7.46582388878"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 126
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Measures sanity check\n",
      "print \"Total number of measures: {}\".format(len(hdf))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total number of measures: 841542\n"
       ]
      }
     ],
     "prompt_number": 127
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Describe the control network\n",
      "\n",
      "What are the summary statistics for all numeric fields?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t1 = time.time()\n",
      "desc = hdf.describe()\n",
      "t2 = time.time()\n",
      "print t2 - t1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4.71718382835\n"
       ]
      }
     ],
     "prompt_number": 133
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Get a subset of the table"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "desc.loc[:,['lineresidual', 'sampleresidual']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>lineresidual</th>\n",
        "      <th>sampleresidual</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>count</th>\n",
        "      <td> 841542.000000</td>\n",
        "      <td> 8.415420e+05</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mean</th>\n",
        "      <td>      0.000003</td>\n",
        "      <td> 3.362980e-09</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>std</th>\n",
        "      <td>      0.167996</td>\n",
        "      <td> 1.186296e-01</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>min</th>\n",
        "      <td>     -5.066055</td>\n",
        "      <td>-6.640318e+00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25%</th>\n",
        "      <td>     -0.092937</td>\n",
        "      <td>-6.806873e-02</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50%</th>\n",
        "      <td>      0.000014</td>\n",
        "      <td>-1.661217e-04</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>75%</th>\n",
        "      <td>      0.092964</td>\n",
        "      <td> 6.753797e-02</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>max</th>\n",
        "      <td>      2.588894</td>\n",
        "      <td> 3.319423e+00</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 135,
       "text": [
        "        lineresidual  sampleresidual\n",
        "count  841542.000000    8.415420e+05\n",
        "mean        0.000003    3.362980e-09\n",
        "std         0.167996    1.186296e-01\n",
        "min        -5.066055   -6.640318e+00\n",
        "25%        -0.092937   -6.806873e-02\n",
        "50%         0.000014   -1.661217e-04\n",
        "75%         0.092964    6.753797e-02\n",
        "max         2.588894    3.319423e+00"
       ]
      }
     ],
     "prompt_number": 135
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Simple, Multi-part query (800,000+ points)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t1 = time.time()\n",
      "hdf.query('lineresidual < -0.09 | lineresidual > 0.09 | sampleresidual < -6.6 | sampleresidual > 0.067')\n",
      "t2 = time.time()\n",
      "print t2 - t1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.367629051208\n"
       ]
      }
     ],
     "prompt_number": 140
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Plot the scatter plot of the subset of the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = hdf['lineresidual'].values\n",
      "y = hdf['sampleresidual'].values\n",
      "\n",
      "\n",
      "sigmas = 3\n",
      "stdx = np.std(x)\n",
      "stdy = np.std(y)\n",
      "\n",
      "meanx = np.mean(x)\n",
      "meany = np.mean(y)\n",
      "\n",
      "xlower = meanx - stdx * sigmas\n",
      "xupper = meanx + stdx * sigmas\n",
      "ylower = meany - stdy * sigmas\n",
      "yupper = meany + stdy * sigmas\n",
      "\n",
      "outlierx = np.where((x < xlower) | (x > xupper))[0]\n",
      "outliery = np.where((y < ylower) | (y > yupper))[0]\n",
      "print len(outlierx), len(outliery)\n",
      "mask = np.intersect1d(outlierx, outliery)\n",
      "        \n",
      "scatter(x[mask], y[mask], alpha=0.5)\n",
      "#hdf.plot(kind='scatter', x='lineresidual', y='sampleresidual')\n",
      "#show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "11500 9922\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 215,
       "text": [
        "<matplotlib.collections.PathCollection at 0x11333e4d0>"
       ]
      }
     ],
     "prompt_number": 215
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Aggregate by the pointid to count histograms"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Aggregate the data by pointid\n",
      "t1 = time.time()\n",
      "grouped = hdf.groupby('pointid')\n",
      "t2 = time.time()\n",
      "print \"Grouping required: {}\".format(t2 - t1)\n",
      "\n",
      "#Get a count of the number of control measures in each control point\n",
      "t1 = time.time()\n",
      "gcounts = grouped.size()\n",
      "t2 = time.time()\n",
      "print \"COUNT required : {}\".format(t2 - t1)\n",
      "\n",
      "hist(gcounts, np.arange(min(gcounts)-1, max(gcounts) + 1 ))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Grouping required: 0.00389409065247\n",
        "COUNT required : 0.844298124313"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 218,
       "text": [
        "(array([  0.00000000e+00,   8.12100000e+04,   8.01540000e+04,\n",
        "          5.49080000e+04,   2.72310000e+04,   9.95600000e+03,\n",
        "          2.78300000e+03,   3.66000000e+02,   8.00000000e+01]),\n",
        " array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
        " <a list of 9 Patch objects>)"
       ]
      }
     ],
     "prompt_number": 218
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print min(gcounts), max(gcounts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2 10\n"
       ]
      }
     ],
     "prompt_number": 161
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rawcounts = gcounts.values\n",
      "minc = np.min(rawcounts)\n",
      "maxc = np.max(rawcounts)\n",
      "median = np.median(rawcounts)\n",
      "std = np.std(rawcounts)\n",
      "hist(rawcounts, np.arange(minc, maxc+1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 166,
       "text": [
        "(array([  8.12100000e+04,   8.01540000e+04,   5.49080000e+04,\n",
        "          2.72310000e+04,   9.95600000e+03,   2.78300000e+03,\n",
        "          3.66000000e+02,   8.00000000e+01]),\n",
        " array([ 2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
        " <a list of 8 Patch objects>)"
       ]
      }
     ],
     "prompt_number": 166
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hist(rawcounts[rawcounts >= 5], np.arange(5, maxc+1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 168,
       "text": [
        "(array([ 27231.,   9956.,   2783.,    366.,     80.]),\n",
        " array([ 5,  6,  7,  8,  9, 10]),\n",
        " <a list of 5 Patch objects>)"
       ]
      }
     ],
     "prompt_number": 168
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Adjacency matrix and graph\n",
      "\n",
      "    1. Which images are connected?\n",
      "    2. By how many points?\n",
      "    \n",
      "This is going to use classic split-apply-combine logic"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t1 = time.time()\n",
      "grouped = hdf.loc[:,['serialnumber', 'pointid']].groupby('serialnumber')\n",
      "t2 = time.time()\n",
      "print t2 - t1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.142441987991\n"
       ]
      }
     ],
     "prompt_number": 203
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Sanity Check - Should be 1124\n",
      "print len(grouped)\n",
      "print grouped"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1124\n",
        "<pandas.core.groupby.DataFrameGroupBy object at 0x11586e4d0>\n"
       ]
      }
     ],
     "prompt_number": 204
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "g = grouped.apply(lambda x: pd.concat((x['pointid'], x['serialnumber'])))\n",
      "k = g.reset_index()\n",
      "k['index'] = k.index\n",
      "k['image'] = k.groupby('serialnumber')['index'].rank()\n",
      "k.pivot_table(rows='serialnumber', columns=('image'), values=0)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "DataError",
       "evalue": "No numeric types to aggregate",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mDataError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-214-8996417a3609>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'serialnumber'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'serialnumber'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/home/jlaura/anaconda/lib/python2.7/site-packages/pandas/util/decorators.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/jlaura/anaconda/lib/python2.7/site-packages/pandas/util/decorators.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/jlaura/anaconda/lib/python2.7/site-packages/pandas/tools/pivot.pyc\u001b[0m in \u001b[0;36mpivot_table\u001b[0;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mgrouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0magged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrouped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/jlaura/anaconda/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_agg_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_iterate_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/jlaura/anaconda/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2586\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2588\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2590\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/jlaura/anaconda/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    685\u001b[0m         \"\"\"\n\u001b[1;32m    686\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cython_agg_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGroupByError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/jlaura/anaconda/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36m_cython_agg_general\u001b[0;34m(self, how, numeric_only)\u001b[0m\n\u001b[1;32m   2506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2507\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_cython_agg_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2508\u001b[0;31m         \u001b[0mnew_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cython_agg_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2509\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_agged_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/home/jlaura/anaconda/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36m_cython_agg_blocks\u001b[0;34m(self, how, numeric_only)\u001b[0m\n\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_blocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2558\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No numeric types to aggregate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2560\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mDataError\u001b[0m: No numeric types to aggregate"
       ]
      }
     ],
     "prompt_number": 214
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 183
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print graph"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "defaultdict(<type 'list'>, {})\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## List intersection\n",
      "Given a list, elist, containing some number of pointids and an hdf5 table representing a control network, find the intersetion of the list with the control network.\n",
      "\n",
      "This is complete timing.  That is: (1) read the hdf5 file from disk, (2) read the text file from disk, (3) query the hdf5 table.  \n",
      "\n",
      "A similar operation using cnetextract required: 04:23:43.0"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t1 = time.time()\n",
      "outfile = 'elysium.h5'\n",
      "hdf = pd.read_hdf(outfile, '/Free/controlmeasure')\n",
      "elist = np.genfromtxt('Elysium_DayIR_Final_PtId_OddPlus.lis', dtype=str)\n",
      "res = hdf['pointid'].isin(elist)\n",
      "t2 = time.time()\n",
      "print t2 - t1\n",
      "\n",
      "print len(hdf[res].groupby('pointid'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8.12126708031\n",
        "129694"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 156
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "cnetextract reports that 129694 points should be left after the set intersection.  \n",
      "\n",
      "Note: The hdf5 representation expands the nested representation, i.e. each row is a control measure and many control measures can have duplicate control point information.  This is not an issue because (1) disk is cheap, (2) hdf5 is chunkable and able to fit into RAM in sections (by extension, chunks can be defined at L1, L2, and (hardware dependent) L3 sizes), (3) compression should be used and control measure information is ripe for compression. \n",
      "\n",
      "Therefore, we must aggregrate the control measures into control points to get an accurate count."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t1 = time.time()\n",
      "len(qobj.groupby('pointid'))\n",
      "t2 = time.time()\n",
      "print t2 - t1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.31991887093\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On my machine, the sum is < 10 seconds with no compression.  I do not image that, from an analytical standpoint, the aggregation is always required"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}